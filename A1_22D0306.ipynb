{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "388313a4",
   "metadata": {},
   "source": [
    "# Assignment 1 for the course EE-769\n",
    "## Roll#: 22D0306\n",
    "### link for recording: https://iitbombay1-my.sharepoint.com/:v:/g/personal/22d0306_iitbombay1_onmicrosoft_com/EVumcQsL4yxAks59LPFhxfQBegf0xF_SfTfE0A4iT5YkJA?e=kpSS9i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f72978a",
   "metadata": {},
   "source": [
    "### Importing the required libraires "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85625091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64609237",
   "metadata": {},
   "source": [
    "## 1. Write a function to generate a data matrix X. \n",
    "Inputs: Number of samples, feature dimension. \n",
    "Output: Data matrix X. [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0839c4e",
   "metadata": {},
   "source": [
    "###  This function generates a matrix of integer random numbers between 1 to 1000. The maximum number of samples can be generated from this function is 1000 while you can change the upper limit by modifying the function. \n",
    "-The sample in the function refers to the number of samples (N) and the dimension refers to the dimension of feature (D).\n",
    "the output of this function will be a matrix of X(NxD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d4ab0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def generate(sample,dimension):\n",
    "    data_X=np.random.randn(sample,dimension)\n",
    "    return (data_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4669a8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.21807867e+00 -8.78136927e-01 -1.73506649e+00  1.58969580e+00\n",
      "   9.01354867e-01]\n",
      " [-3.58003166e-01 -9.85039033e-01  2.18461679e-02  1.17450208e+00\n",
      "   7.05225136e-01]\n",
      " [-7.49360558e-02  1.27445294e-01  1.00866665e+00 -1.73891551e-01\n",
      "  -1.78967621e-01]\n",
      " ...\n",
      " [ 5.77325565e-01 -6.53682895e-02  6.39998637e-01  2.07773175e+00\n",
      "   4.51903629e-02]\n",
      " [-1.47673367e+00  7.95700822e-01 -1.65337040e-04 -8.17964748e-01\n",
      "   2.28874291e+00]\n",
      " [-2.31590859e-01 -7.57168876e-01 -1.02650958e+00 -4.39289377e-01\n",
      "   1.91993568e+00]] (500, 5)\n"
     ]
    }
   ],
   "source": [
    "data_X=generate(500,5)\n",
    "print (data_X,data_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514912aa",
   "metadata": {},
   "source": [
    "# 2. Write a function to generate dependent variable column t.  [1]\n",
    "a) Inputs: Data matrix X, weight vector for each column, bias w0, noise variance\n",
    "\n",
    "b) Output: Target vector t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654c67a9",
   "metadata": {},
   "source": [
    "### a. Generating the inputs\n",
    "The function below takes the number of sample and dimension as input and gerenates the random numbers. According to the number of samples and dimension we can specify if we want to get a matrix of random number or a singel dimensional vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e020cc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_input(sample,dimension):\n",
    "    weight1=np.random.rand(1,dimension)\n",
    "    w0_bias=np.random.rand(1,1)\n",
    "    v_noise=np.random.randn(sample,1)\n",
    "    #for normalizing the noise value among different methods I have chosen to divide all the values by\n",
    "    #the maximum value so the new values of v will me v/max(v)\n",
    "    v_noise=v_noise/max(v_noise)\n",
    "    data=np.random.randn(sample, dimension)\n",
    "    return (weight1,w0_bias,v_noise,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a90ac949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Values\n",
      " (1, 6) [[0.64539423 0.77605604 0.11902847 0.07519482 0.74512487 0.03706018]] \n",
      " Bias value:\n",
      " (1, 1) [[0.59134986]] \n",
      " Noise values :\n",
      " (1000, 1) [-0.41653712 -0.0567978   0.11092214 -0.05599268 -0.28324961] \n",
      " X matrix:\n",
      " (1000, 6) [[-0.82293785 -0.15201303  0.38640492 -0.10122028]\n",
      " [ 0.71297086 -0.85944296 -1.07358667  0.31189588]\n",
      " [-0.09413607  0.88828715  0.91039804  0.27836948]\n",
      " [-0.93821492  0.81505626  0.51629904 -1.05969622]\n",
      " [ 0.2868607   0.98841409 -1.36241645 -0.76484339]]\n"
     ]
    }
   ],
   "source": [
    "w1,bias,noise,X=gen_input(1000,6)\n",
    "print (\"Weight Values\\n\",w1.shape,\n",
    "       w1,\"\\n\",\"Bias value:\\n\",bias.shape,\n",
    "       bias,\"\\n\",\"Noise values :\\n\",noise.shape,\n",
    "       noise[0:5,0],\"\\n\",\"X matrix:\\n\",X.shape,\n",
    "       X[0:5,0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94db128f",
   "metadata": {},
   "source": [
    "### b.Function for generating the dependent value t\n",
    "The function below is for generating the target values using the inputs data, weight values, bias value and noise values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89131ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dep(data,weight,bias,noise):\n",
    "    return ((bias+data.dot(weight.T)+noise))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d21b8378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1) [0.44075174 1.22141974 1.13873639 0.31221324 0.77679901 0.01825442\n",
      " 0.3735528  1.77210285 0.6439075  0.24243874]\n"
     ]
    }
   ],
   "source": [
    "target=gen_dep(X,w1,bias,noise)\n",
    "print (target.shape,target[0:10,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe9a756",
   "metadata": {},
   "source": [
    "# 3. Write a function to compute a linear regression estimate. \n",
    "a) input: data matrix X and weight vector w\n",
    "\n",
    "b) Output: y\n",
    "\n",
    "The function below only gets weight, data and bias as inputs and calculate the predicted values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bec7beaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_prediction(data,weight,bias):\n",
    "    y=bias+weight.dot(data.T)\n",
    "    return (y.T)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c39f057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1) [0.85728886 1.27821754 1.02781425 0.36820592 1.06004862 0.04449287\n",
      " 0.2875385  1.46779009 0.5075067  0.39985779]\n"
     ]
    }
   ],
   "source": [
    "y=Linear_prediction(X,w1,bias)\n",
    "print (y.shape,y[0:10,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16872dab",
   "metadata": {},
   "source": [
    "# 4. Write a function to compute the mean square error of two vectors y and t. \n",
    "\n",
    "Mean square error of predicted and observed values are calculated as: (t-y)^2/N, where \"t\" is the observed or targeted values and \"y\" is the predicted values and \"N\" is the lenght of values. Through out this assignment we will be using the equation mentioned to compute the MSE values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b47b8dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE (y,t):\n",
    "    mean_sq_error=np.sum(np.square(t-y))/t.shape[0]\n",
    "    return (mean_sq_error)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "836dc5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08187484305697595\n"
     ]
    }
   ],
   "source": [
    "me=MSE(y,target)\n",
    "print (me)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaf3c9a",
   "metadata": {},
   "source": [
    "# 5. Write a function to estimate the weights of linear regression using pseudo-inverse, assuming L2 regularization:\n",
    "a) input: X, t, and lambda\n",
    "b) output: w, MSE, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668b13f2",
   "metadata": {},
   "source": [
    "### Function description:\n",
    "The function below calculates the weight of a regression problem using pseudo inverse assuming L2 normalization, using the weight caluculated in the first line then the function calculates the predicted value and finally the mse is calculated between the predicted and targeted values. \n",
    "\n",
    "***First we will try the function without including the bias (w0) value and then we will try the function including the w0 values and observe the changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18509975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pinv_w_est (D,t,lamb):\n",
    "    D_pinv=np.linalg.pinv(lamb*np.identity(D.shape[1])+(D.T@D))\n",
    "    weight=(D_pinv.dot(D.T@t)).T\n",
    "    predicted_y=(weight.dot(D.T)).T\n",
    "    mean_sq_error=MSE(predicted_y,t)\n",
    "    return (weight,predicted_y,mean_sq_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "699323fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weight values:\n",
      " [[0.63470519 0.77651895 0.15855659 0.08653736 0.75580214 0.04548366]] \n",
      " The predicted values\n",
      " [[ 0.30597058]\n",
      " [ 0.65703126]\n",
      " [ 0.48934734]\n",
      " [-0.21680362]\n",
      " [ 0.41066825]] \n",
      " The Mean square error\n",
      " 0.42853197322247266\n"
     ]
    }
   ],
   "source": [
    "#Finding the weights without bias value (w0).\n",
    "w,y,mse=Pinv_w_est (X,target,0.01)\n",
    "print(\"The weight values:\\n\",w,\"\\n\",\n",
    "      \"The predicted values\\n\",y[0:5,:],\"\\n\",\n",
    "      \"The Mean square error\\n\",mse)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be89eda",
   "metadata": {},
   "source": [
    "### Including W0\n",
    "For finding the w0  using pseudo inverse we can add a new column of all ones and then use it for calculating the weight values, the first value of weight vector will w0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e58149d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Including the bias value\n",
    "XX=np.concatenate([np.ones([X.shape[0],1]),X],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42f85e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weight values:\n",
      " [[0.59027889 0.64656268 0.77985994 0.13890067 0.06848194 0.75650669\n",
      "  0.02938024]] \n",
      " The predicted values\n",
      " [[0.87160234]\n",
      " [1.26362225]\n",
      " [1.023639  ]\n",
      " [0.39251001]\n",
      " [1.02572729]\n",
      " [0.0235856 ]\n",
      " [0.294102  ]\n",
      " [1.43900522]] \n",
      " The Mean square error\n",
      " 0.08120413248579261\n"
     ]
    }
   ],
   "source": [
    "w_w0,y,mse=Pinv_w_est (XX,target,0.01)\n",
    "print(\"The weight values:\\n\",w_w0,\"\\n\",\n",
    "      \"The predicted values\\n\",y[0:8,:],\"\\n\",\n",
    "      \"The Mean square error\\n\",mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af626b7",
   "metadata": {},
   "source": [
    "# 6. Write a function to compute the gradient of MSE with respect to its weight vector. \n",
    "\n",
    "a) Input: X matrix, t vector, and w vector\n",
    "\n",
    "b) Output: gradient vector\n",
    "\n",
    "The function below take data matrix, target vector and weight values as input and calculates the gradient of MSE respected to weight. The function first calculates the predicted y and calculates the gradient using the equation:MSE=(t-y)^2/N, y=x*w so, MSE=(t-w*x)^2/N, and dMSE/dw=-2*(t-w*x)(x)/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cc80823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_mse(data_input, target, weight):\n",
    "    y_pred=data_input.dot(weight.T)\n",
    "    gradient=-2*(X.T@(target-y_pred))/target.shape[0]\n",
    "    return (gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "657249f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.26941038e-05],\n",
       "       [-1.55303790e-05],\n",
       "       [-3.17113183e-06],\n",
       "       [-1.73074715e-06],\n",
       "       [-1.51160428e-05],\n",
       "       [-9.09673250e-07]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad=grad_mse(X, target, w)\n",
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f543239",
   "metadata": {},
   "source": [
    "# 7. Write a function to compute L2 norm of a vector w passed as a numpy array. Exclude bias w0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa112c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_nor(weight):\n",
    "    return np.sum(np.square(weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2ef3f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6117669046225043"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_nor(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5947bb76",
   "metadata": {},
   "source": [
    "# 8. Write a function to compute the gradient of L2 norm with respect to the weight vectors. \n",
    "\n",
    "a) Input: X matrix and w vector\n",
    "\n",
    "b) Output: gradient vector, where gradient with respect to w0 is 0.\n",
    "\n",
    "L2 norm= sum(w)^2, dL2/dw=2*sum(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed12bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_norm_grad(data,weight):\n",
    "    gradient=2*np.sum(weight)\n",
    "    return(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04873f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.915207785015548"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_norm_grad(X,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b286df",
   "metadata": {},
   "source": [
    "# 9. Write a function to compute L1 norm of a vector w passed as a numpy array. Exclude bias w0. [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4665972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_L1norm(weight,lambda1):\n",
    "    return(lambda1*np.abs(weight))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07aa4a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.34705189e-04 7.76518951e-04 1.58556592e-04 8.65373577e-05\n",
      "  7.55802140e-04 4.54836625e-05]]\n"
     ]
    }
   ],
   "source": [
    "w_norm=cal_L1norm(weight=w,lambda1=0.001)\n",
    "print (w_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcfe64e",
   "metadata": {},
   "source": [
    "# 10. Write a function to compute the gradient of L1 norm with respect to the weight vectors. [2]\n",
    "\n",
    "a) Input: X matrix and w vector\n",
    "\n",
    "b) Output: gradient vector, where gradient with respect to w0 is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08373d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1_norm_grad(data,weight):\n",
    "    gradient=np.sign(weight)\n",
    "    return(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ed114f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L1_norm_grad(X,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de9d0d0",
   "metadata": {},
   "source": [
    "# 11. Write a function for a single update of weights of linear regression using gradient descent. [2]\n",
    "\n",
    "a) Input: X, t, w, eta, lambda 2, lambda 1. Note that the weight of MSE will be 1\n",
    "\n",
    "b) Output: updated weight and updated MSE\n",
    "\n",
    "The function calculates the new weight values using gradient descent for one iteration only. First we will calculate the gradient of a given data, target values and weight, then we will subtract the gradient from the old w values and store it in new w values. After that we will predict the y values using the new w values and calculate the new MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db05c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_reg(data,t,w,eta,lamb):\n",
    "    grad=grad_mse(data, t, w)\n",
    "    w_new=np.zeros([1,w.shape[1]])\n",
    "    for i in range(w.shape[1]):\n",
    "        w_new[0,i]=w[0,i]-eta*grad[i,0]\n",
    "    ypred=Linear_prediction(data,w_new,bias=0)\n",
    "    mse=MSE(ypred,target)\n",
    "    return (w_new, mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "468b7a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_new,mse_new=linear_reg(X,target,w,0.0001,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f509846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w new values:\n",
      " [[0.63470519 0.77651895 0.15855659 0.08653736 0.75580214 0.04548366]] \n",
      "\n",
      " MSE new values:\n",
      " 0.42853197322240816\n"
     ]
    }
   ],
   "source": [
    "print(\"w new values:\\n\",w_new,\"\\n\\n\",\"MSE new values:\\n\",mse_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69c11f1",
   "metadata": {},
   "source": [
    "# 12. Write a function to estimate the weights of linear regression using gradient descent. [3]\n",
    "\n",
    "a) Inputs: X, t, lambda2 (default 0), lambda1 (default 0), eta, max_iter, min_change_NRMSE\n",
    "\n",
    "b) Output: Final w, final RMSE normalized with respect to variance of t.\n",
    "\n",
    "The function is same as question 11 but with some more parameters as inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78e49653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_reg_w(data,t,eta, max_iter, min_change_NRMSE,lam2=0, lam1=0):\n",
    "    n, d = data.shape\n",
    "    weight = np.ones([d,1])\n",
    "    for i in range(max_iter):\n",
    "        y = np.dot(data, weight)\n",
    "        grad = (-2*(data.T@(t-y))/t.shape[0]) + lam2 * np.square(weight)  + lam1 * np.abs(weight)\n",
    "        weight -= eta * grad\n",
    "        NRMSE = np.sqrt(np.mean(np.square(y - t))) / (np.max(t) - np.min(t))\n",
    "        if NRMSE < min_change_NRMSE:\n",
    "            break\n",
    "    return weight,y, NRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d19020f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.61285184],\n",
       "        [0.74418351],\n",
       "        [0.15628956],\n",
       "        [0.0855384 ],\n",
       "        [0.72608249],\n",
       "        [0.04571396]]),\n",
       " 0.07663635354307573)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,y,NRMSE=linear_reg_w(X,target,lam2=0.1, lam1=0.01,eta=0.001, max_iter=10000, min_change_NRMSE=0.0001)\n",
    "w,NRMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afea44a4",
   "metadata": {},
   "source": [
    "# 13. Run multiple experiments (with different random seeds) for, plot the results of (box plots), and comment on the trends and potential reasons for the following relations:\n",
    "\n",
    "a) Training and validation NRMSE obtained using pseudo inverse with number of training samples [2]\n",
    "\n",
    "b) Training and validation NRMSE obtained using pseudo inverse with number of variables [2]\n",
    "\n",
    "c) Training and validation NRMSE obtained using pseudo inverse with noise variance [2]\n",
    "\n",
    "d) Training and validation NRMSE obtained using pseudo inverse with w0 [2]\n",
    "\n",
    "e) Training and validation NRMSE obtained using pseudo inverse with lambda2 [2]\n",
    "\n",
    "f) Time taken to solve pseudo inverse with number of samples and number of variables and its breaking points [2]\n",
    "\n",
    "g) Training and validation NRMSE obtained using gradient descent with max_iter [2]\n",
    "\n",
    "h) Training and validation NRMSE obtained using gradient descent with eta [2]\n",
    "\n",
    "i) Time taken to solve gradient descent with number of samples and number of variables and its breaking points [2]\n",
    "\n",
    "j) Time taken to solve gradient descent with number of variables and its breaking point [2]\n",
    "\n",
    "k) Training and validation NRMSE and number of nearly zero weights obtained using gradient descent with lambda2 [2]\n",
    "\n",
    "l) Training and validation NRMSE and number of nearly zero weights obtained using gradient descent with lambda1 [2]\n",
    "\n",
    "m) Training and validation NRMSE for optimal lambda2 with noise variance [2]\n",
    "\n",
    "n) Training and validation NRMSE for optimal lambda1 with noise variance [2]\n",
    "\n",
    "o) Experiment (f) but, this time with number of training samples and number of variables [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48a2c1f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0237735  0.02977773]\n",
      " [0.08281987 0.13331837]\n",
      " [0.07002175 0.1201246 ]\n",
      " [0.03222216 0.03931113]\n",
      " [0.0321143  0.03397429]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x15f3cd491c0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3cd494c0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3cd58610>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3cd588e0>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x15f3cd49850>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3cd49a60>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3cd58bb0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3cd58e80>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x15f3cd49040>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3cd58340>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x15f3cd49d30>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3cd66190>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x15f3cd58040>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3cd66460>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a) Training and validation NRMSE obtained using pseudo inverse with number of training samples\n",
    "\n",
    "sample=np.array([500,1000,1500,2000,2500])\n",
    "NRMSE=np.zeros([sample.shape[0],2])\n",
    "for i in range(sample.shape[0]):\n",
    "    samp=sample[i]\n",
    "    weight1,w0_bias,v_noise,data=gen_input(samp,dimension=6)\n",
    "    target=gen_dep(data,weight1,w0_bias,v_noise)\n",
    "    \n",
    "    train_tar_set=target[0:np.int32(np.round(0.75*target.shape[0])),:]\n",
    "    val_tar_set=target[np.int32(np.round(0.75*target.shape[0]))+1:np.int32(np.round(0.85*target.shape[0])),:]\n",
    "    train_set=data[0:np.int32(np.round(0.75*data.shape[0])),:]\n",
    "    val_set=data[np.int32(np.round(0.75*data.shape[0]))+1:np.int32(np.round(0.85*data.shape[0])),:]\n",
    "    \n",
    "    weight,predicted_y,mean_sq_error=Pinv_w_est (train_set,train_tar_set,lamb=0)\n",
    "    NRMSE[i,0]=np.sqrt(mean_sq_error)/(np.max(train_tar_set)-np.min(train_tar_set))\n",
    "    \n",
    "    new_y=Linear_prediction(val_set,weight,bias=0)\n",
    "    NRMSE[i,1]=np.sqrt(MSE(new_y,val_tar_set))/(np.max(val_tar_set)-np.min(val_tar_set))\n",
    "    \n",
    "    del weight1,w0_bias,v_noise,data,weight,predicted_y,mean_sq_error\n",
    "print(NRMSE)\n",
    "plt.boxplot(NRMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7180695",
   "metadata": {},
   "source": [
    "### Comments [a]:\n",
    "Number of samples have always been an important factor on accuracy of the model but also sometimes can cause poor accuracy due to presence of outliers. In the present case the accuracy is good when number of samples are 500 but decreases when it increases and again decreases. As we are generating numbers randomly the more number we are generating can increase the outliers but as the number of samples are much more higher then the difference between the numbers also decreases and the number of outliers will decrease. The model fitting is good in case of samples [500,2000,2500] but has overfitting in case of [1000,1500]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f19d965",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14278325 0.14165757]\n",
      " [0.06960297 0.07159643]\n",
      " [0.08298289 0.10047682]\n",
      " [0.04413481 0.04587982]\n",
      " [0.05713292 0.06513163]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x15f3d6a61c0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3d6a6490>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3d6b45b0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3d6b4880>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x15f3d6a6760>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3d6a6a30>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3d6b4b50>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3d6b4e20>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x15f3d695eb0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3d6b42e0>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x15f3d6a6d00>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3d6c1130>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x15f3d6a6fd0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3d6c1400>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# b) Training and validation NRMSE obtained using pseudo inverse with number of variables [2]\n",
    "Dimension=np.array([2,5,10,15,20])\n",
    "NRMSE=np.zeros([Dimension.shape[0],2])\n",
    "for i in range(Dimension.shape[0]):\n",
    "    \n",
    "    sample=1000\n",
    "    weight1,w0_bias,v_noise,data=gen_input(sample,Dimension[i])\n",
    "    target=gen_dep(data,weight1,w0_bias,v_noise)\n",
    "    \n",
    "    train_tar_set=target[0:np.int32(np.round(0.75*target.shape[0])),:]\n",
    "    val_tar_set=target[np.int32(np.round(0.75*target.shape[0]))+1:np.int32(np.round(0.85*target.shape[0])),:]\n",
    "    train_set=data[0:np.int32(np.round(0.75*data.shape[0])),:]\n",
    "    val_set=data[np.int32(np.round(0.75*data.shape[0]))+1:np.int32(np.round(0.85*data.shape[0])),:]\n",
    "    \n",
    "    # estimation of the weight and prediction for training data\n",
    "    weight,predicted_y,mean_sq_error=Pinv_w_est (train_set,train_tar_set,lamb=0)\n",
    "    NRMSE[i,0]=np.sqrt(mean_sq_error)/(np.max(train_tar_set)-np.min(train_tar_set))\n",
    "    \n",
    "    # estimation of the weight and prediction for validation data\n",
    "    weight2,predicted_y2,mean_sq_error2=Pinv_w_est (val_set,val_tar_set,lamb=0)\n",
    "    NRMSE[i,1]=np.sqrt(mean_sq_error2)/(np.max(val_tar_set)-np.min(val_tar_set))\n",
    "    \n",
    "    \n",
    "    del weight1,w0_bias,v_noise,data,weight,predicted_y,mean_sq_error\n",
    "print(NRMSE)\n",
    "plt.boxplot(NRMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07032eca",
   "metadata": {},
   "source": [
    "### Comment [b]:\n",
    "Number of dimensions is also an important factor on accuracy of the model. Here the accuracy increases till some level and again it decreases for a higher number of dimensions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03cb3234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06050463 0.07155546]\n",
      " [0.06579229 0.08142298]\n",
      " [0.08666426 0.13186963]\n",
      " [0.0614565  0.06929015]\n",
      " [0.03461024 0.04874586]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x15f3d725130>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3d725400>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3d735400>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3d7356d0>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x15f3d7255b0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3d725880>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3d7359a0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3d735c70>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x15f3d718e20>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3d735130>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x15f3d725b50>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3d735f40>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x15f3d725e20>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3d73f250>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAO9klEQVR4nO3dUYgch33H8d+v6zNXN3HtwwdxJbnSg0hWLC02i2uaI3BJA5Zbqjz0wQeJibsg/OC1XRqK231o+nDQhxDaHMZC+FQwNeuHxA+imLqFLpQF2+ikhOTkw3CoTXW1gi9Y2GnMRavzvw+3Vs/XlW5Ovr3R/e/7gcXamdnd/4L4epgZ7TgiBADI69fKHgAAMFyEHgCSI/QAkByhB4DkCD0AJHdb2QMMcs8998TBgwfLHgMAdo2zZ8/+PCLGB627JUN/8OBBzc3NlT0GAOwatn96vXUcugGA5Ag9ACRH6AEgOUIPAMkRegBIjtAD2HHtdlu1Wk2VSkW1Wk3tdrvskVK7JS+vBJBXu91Wq9XS7OysJiYm1O121Wg0JElTU1MlT5eTb8WfKa7X68F19EBOtVpNMzMzmpycvLas0+mo2Wxqfn6+xMl2N9tnI6I+cB2hB7CTKpWKVlZWNDIycm1Zr9fT6OioVldXS5xsd7tR6DlGD2BHVatVdbvdTyzrdruqVqslTZQfoQewo1qtlhqNhjqdjnq9njqdjhqNhlqtVtmjpcXJWAA76uMTrs1mUwsLC6pWq5qenuZE7BBxjB4AEuAYPQDsYYQeAJIj9ACQHKEHgOQIPQAkR+gBIDlCDwDJEXoASI7QA0ByhB4AkiP0AJAcoQeA5Ag9ACRH6AEgOUIPAMkRegBIrlDobT9s+23bi7afHbD+C7Zft/0r299at/yA7Y7tBdvnbT+9ncMDADa36a0EbVckPSfpq5KWJJ2xfToi3lq32XuSnpL0tQ0vvyrpzyPinO3PSjpr+183vBYAMERF9ugflLQYERci4oqklyUdW79BRLwbEWck9TYsvxQR5/p//oWkBUn7tmVyAEAhRUK/T9LFdc+XdBOxtn1Q0v2S3rzO+uO252zPLS8vb/XtAQDXUST0HrBsS3cUt/0ZST+Q9ExEfDBom4g4GRH1iKiPj49v5e0BADdQJPRLkg6se75f0jtFP8D2iNYi/1JEvLK18QAAn1aR0J+RdNj2Idu3S3pU0ukib27bkmYlLUTEd29+TADAzdr0qpuIuGr7SUmvSapIOhUR520/0V9/wvbnJM1JulPSR7afkXRE0u9I+oakn9j+Uf8t/yoiXt32bwIAGGjT0EtSP8yvblh2Yt2ff6a1QzobdTX4GD8AYIfwL2MBIDlCDwDJEXoASI7QA0ByhB4AkiP0AJAcoQeA5Ag9ACRH6AEgOUIPAMkRegBIjtADQHKEHgCSI/QAkByhB4DkCD0AJEfoASA5Qg8AyRF6AEiO0ANAcoQeAJIj9ACQHKEHgOQIPQAkR+gBIDlCDwDJEXoASI7QA0ByhB4AkiP0AJAcoQeA5Ag9ACRXKPS2H7b9tu1F288OWP8F26/b/pXtb23ltQCA4do09LYrkp6TdFTSEUlTto9s2Ow9SU9J+s5NvBYAMERF9ugflLQYERci4oqklyUdW79BRLwbEWck9bb6WgDAcBUJ/T5JF9c9X+ovK6Lwa20ftz1ne255ebng2wMANlMk9B6wLAq+f+HXRsTJiKhHRH18fLzg2wMANlMk9EuSDqx7vl/SOwXf/9O8FgCwDYqE/oykw7YP2b5d0qOSThd8/0/zWgDANrhtsw0i4qrtJyW9Jqki6VREnLf9RH/9CdufkzQn6U5JH9l+RtKRiPhg0GuH9F0AAAM4oujh9p1Tr9djbm6u7DEAYNewfTYi6oPW8S9jASA5Qg8AyRF6AEiO0ANAcoQeAJIj9ACQHKEHgOQIPQAkR+gBIDlCDwDJEXoASI7QA0ByhB4AkiP0AJAcoQeA5Ag9ACRH6AEgOUIPAMkR+qTa7bZqtZoqlYpqtZra7XbZIwEoyaY3B8fu02631Wq1NDs7q4mJCXW7XTUaDUnS1NRUydMB2GncHDyhWq2mmZkZTU5OXlvW6XTUbDY1Pz9f4mQAhuVGNwcn9AlVKhWtrKxoZGTk2rJer6fR0VGtrq6WOBmAYblR6DlGn1C1WlW32/3Esm63q2q1WtJEAMpE6BNqtVpqNBrqdDrq9XrqdDpqNBpqtVpljwagBJyMTejjE67NZlMLCwuqVquanp7mRCywR3GMHgAS4Bg9AOxhhB4AkiP0AJAcoQeA5Ag9ACRH6AEguUKht/2w7bdtL9p+dsB62/5ef/2PbT+wbt2f2T5ve9522/bodn4BALeGsbEx2R7qY2xsrOyvuSttGnrbFUnPSToq6YikKdtHNmx2VNLh/uO4pOf7r90n6SlJ9YioSapIenTbpgdwy7h8+bIiYqiPy5cvl/01d6Uie/QPSlqMiAsRcUXSy5KObdjmmKQXY80bku6yfW9/3W2Sft32bZLukPTONs0OACigSOj3Sbq47vlSf9mm20TEf0v6jqT/knRJ0vsR8S+DPsT2cdtztueWl5eLzg8A2ESR0HvAso2/mzBwG9t3a21v/5Ck35L0G7a/PuhDIuJkRNQjoj4+Pl5gLABAEUVCvyTpwLrn+/X/D79cb5s/kPQfEbEcET1Jr0j6/ZsfFwCwVUVCf0bSYduHbN+utZOppzdsc1rSY/2rbx7S2iGaS1o7ZPOQ7TtsW9JXJC1s4/wAgE1s+jPFEXHV9pOSXtPaVTOnIuK87Sf6609IelXSI5IWJX0o6fH+ujdtf1/SOUlXJf1Q0slhfBEAwGD8TDGAbWFbw+7JTnzGbsXPFAPAHkboASA5Qg8AyRF6AEiO0ANAcoQeAJIj9ACQHKEHgOQIPQAkR+gBIDlCDwDJbfqjZgBQRPz1ndK3f3P4n4EtI/QAtoX/5oOd+VGzbw/1I1Li0A0AJEfoASA5Dt0A2DZrN5Ibnrvvvnuo758VoQewLbghyK2LQzcAkByhB4DkCD0AJEfoASA5Qg8AyRF6AEiO0O8yY2Njsj3Ux9jYWNlfE8A24jr6Xeby5cs78nsiAPJgjx4AkiP0AJAcoQeA5Ag9ACRH6AEgOUIPAMkVurzS9sOS/l5SRdILEfG3G9a7v/4RSR9K+mZEnOuvu0vSC5JqkkLSn0bE69v1BfYa7ssJYKs2Db3tiqTnJH1V0pKkM7ZPR8Rb6zY7Kulw//F7kp7v/1da+x/AP0fEn9i+XdId2zj/nsN9OQFsVZE9+gclLUbEBUmy/bKkY5LWh/6YpBdjrUBv2L7L9r2SfinpS5K+KUkRcUXSle0bf2/iLj4AtqJI6PdJurju+ZL+b2/9Rtvsk3RV0rKkf7D9u5LOSno6In658UNsH5d0XJLuu+++ovPvOdzFB8BWFTkZO2j3cWNtrrfNbZIekPR8RNyvtT38Zwd9SEScjIh6RNTHx8cLjAUAKKJI6JckHVj3fL+kdwpusyRpKSLe7C//vtbCDwDYIUVCf0bSYduH+idTH5V0esM2pyU95jUPSXo/Ii5FxM8kXbT9+f52X9Enj+0DAIZs02P0EXHV9pOSXtPa5ZWnIuK87Sf6609IelVrl1Yuau3yysfXvUVT0kv9/0lc2LAOADBkvhVP7tXr9Zibmyt7DADYNWyfjYj6oHX8y1gASI7QA0ByhB4AkiP0AJAcoQeA5Ag9ACRH6AEgOUIPAMkRegBIjtADQHKEHgCSI/QAkByhB4DkCD0AJEfoAey4drutWq2mSqWiWq2mdrtd9kipFbk5OABsm3a7rVarpdnZWU1MTKjb7arRaEiSpqamSp4uJ248AmBH1Wo1zczMaHJy8tqyTqejZrOp+fn5Eifb3W504xFCD2BHVSoVraysaGRk5NqyXq+n0dFRra6uljjZ7sYdpgDcMqrVqrrd7ieWdbtdVavVkibKj9AD2FGtVkuNRkOdTke9Xk+dTkeNRkOtVqvs0dLiZCyAHfXxCddms6mFhQVVq1VNT09zInaIOEYPAAlwjB4A9jBCDwDJEXoASI7QA0ByhB4AkiP0AJAcoQeA5Ag9ACRH6AEgOUIPAMkVCr3th22/bXvR9rMD1tv29/rrf2z7gQ3rK7Z/aPuftmtwAEAxm4bedkXSc5KOSjoiacr2kQ2bHZV0uP84Lun5DeuflrTwqacFAGxZkT36ByUtRsSFiLgi6WVJxzZsc0zSi7HmDUl32b5Xkmzvl/SHkl7YxrkBAAUVCf0+SRfXPV/qLyu6zd9J+gtJH93oQ2wftz1ne255ebnAWACAIoqE3gOWbfxt44Hb2P4jSe9GxNnNPiQiTkZEPSLq4+PjBcYCABRRJPRLkg6se75f0jsFt/mipD+2/Z9aO+TzZdv/eNPTAgC2rEjoz0g6bPuQ7dslPSrp9IZtTkt6rH/1zUOS3o+ISxHxlxGxPyIO9l/3bxHx9e38AgCAG9v0VoIRcdX2k5Jek1SRdCoiztt+or/+hKRXJT0iaVHSh5IeH97IAICt4FaCAJAAtxIEgD2M0ANAcoQ+qXa7rVqtpkqlolqtpna7XfZIAEqy6clY7D7tdlutVkuzs7OamJhQt9tVo9GQJE1NTZU8HYCdxsnYhGq1mmZmZjQ5OXltWafTUbPZ1Pz8fImTARiWG52MJfQJVSoVraysaGRk5NqyXq+n0dFRra6uljgZgGHhqps9plqtqtvtfmJZt9tVtVotaSIAZSL0CbVaLTUaDXU6HfV6PXU6HTUaDbVarbJHA1ACTsYm9PEJ12azqYWFBVWrVU1PT3MiFtijOEYPAAlwjB4A9jBCDwDJEXoASI7QA0ByhB4Akrslr7qxvSzpp2XPkcQ9kn5e9hDAdfD3c/v8dkQMvOH2LRl6bB/bc9e75AooG38/dwaHbgAgOUIPAMkR+vxOlj0AcAP8/dwBHKMHgOTYoweA5Ag9ACRH6JOyfcr2u7a5dyBuKbYP2O7YXrB93vbTZc+UHcfok7L9JUn/I+nFiKiVPQ/wMdv3Sro3Is7Z/qyks5K+FhFvlTxaWuzRJxUR/y7pvbLnADaKiEsRca7/519IWpC0r9ypciP0AEpj+6Ck+yW9WfIoqRF6AKWw/RlJP5D0TER8UPY8mRF6ADvO9ojWIv9SRLxS9jzZEXoAO8q2Jc1KWoiI75Y9z15A6JOy3Zb0uqTP216y3Sh7JqDvi5K+IenLtn/UfzxS9lCZcXklACTHHj0AJEfoASA5Qg8AyRF6AEiO0ANAcoQeAJIj9ACQ3P8C2ji6lIqeNskAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# c) Training and validation NRMSE obtained using pseudo inverse with noise variance [2]\n",
    "Dimension=np.array([2,5,10,15,20])\n",
    "NRMSE=np.zeros([Dimension.shape[0],2])\n",
    "for i in range(Dimension.shape[0]):\n",
    "    \n",
    "    sample=1000\n",
    "    weight1,w0_bias,v_noise,data=gen_input(sample,Dimension[i])\n",
    "    target=gen_dep(data,weight1,w0_bias,v_noise)\n",
    "    \n",
    "    train_tar_set=target[0:np.int32(np.round(0.75*target.shape[0])),:]\n",
    "    val_tar_set=target[np.int32(np.round(0.75*target.shape[0]))+1:np.int32(np.round(0.85*target.shape[0])),:]\n",
    "    train_set=data[0:np.int32(np.round(0.75*data.shape[0])),:]\n",
    "    val_set=data[np.int32(np.round(0.75*data.shape[0]))+1:np.int32(np.round(0.85*data.shape[0])),:]\n",
    "    \n",
    "    # estimation of the weight and prediction for training data\n",
    "    weight,predicted_y,mean_sq_error=Pinv_w_est (train_set,train_tar_set,lamb=0)\n",
    "    NRMSE[i,0]=np.sqrt(mean_sq_error)/(np.max(train_tar_set)-np.min(train_tar_set))\n",
    "    \n",
    "    # estimation of the weight and prediction for validation data\n",
    "    weight2,predicted_y2,mean_sq_error2=Pinv_w_est (val_set,val_tar_set,lamb=0)\n",
    "    NRMSE[i,1]=np.sqrt(mean_sq_error2)/(np.max(val_tar_set)-np.min(val_tar_set))\n",
    "    \n",
    "    \n",
    "    del weight1,w0_bias,v_noise,data,weight,predicted_y,mean_sq_error\n",
    "print(NRMSE)\n",
    "plt.boxplot(NRMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5129d660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01978648 0.02703803]]\n"
     ]
    }
   ],
   "source": [
    "# d) Training and validation NRMSE obtained using pseudo inverse with w0 [2]\n",
    "\n",
    "NRMSE=np.zeros([1,2])\n",
    "    \n",
    "    \n",
    "weight1,w0_bias,v_noise,data=gen_input(sample=1000,dimension=20)\n",
    "\n",
    "# adding a column of all ones to data for calculating the w0:\n",
    "data1=np.concatenate((np.ones([data.shape[0],1]),data),axis=1)\n",
    "\n",
    "#Calculating the target values\n",
    "target=gen_dep(data,weight1,w0_bias,v_noise)\n",
    "\n",
    "#splitting the data into training and validation\n",
    "train_tar_set=target[0:np.int32(np.round(0.75*target.shape[0])),:]\n",
    "val_tar_set=target[np.int32(np.round(0.75*target.shape[0]))+1:np.int32(np.round(0.85*target.shape[0])),:]\n",
    "train_set=data1[0:np.int32(np.round(0.75*data1.shape[0])),:]\n",
    "val_set=data1[np.int32(np.round(0.75*data1.shape[0]))+1:np.int32(np.round(0.85*data1.shape[0])),:]\n",
    "    \n",
    "# estimation of the weight and prediction for training data\n",
    "weight,predicted_y,mean_sq_error=Pinv_w_est (train_set,train_tar_set,lamb=0)\n",
    "NRMSE[0,0]=np.sqrt(mean_sq_error)/(np.max(train_tar_set)-np.min(train_tar_set))\n",
    "    \n",
    "# estimation of the weight and prediction for validation data\n",
    "weight2,predicted_y2,mean_sq_error2=Pinv_w_est (val_set,val_tar_set,lamb=0)\n",
    "NRMSE[0,1]=np.sqrt(mean_sq_error2)/(np.max(val_tar_set)-np.min(val_tar_set))\n",
    "print (NRMSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729bce99",
   "metadata": {},
   "source": [
    "### Comments [d]:\n",
    "Including W0 on weight calculation has a huge impact on accuracy of the model. On previous analysis the NRMSE has never reached to 0.019 for training and 0.027 for validation data even though we have changed the number of samples and dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "40ba2e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04573049 0.05401633]]\n"
     ]
    }
   ],
   "source": [
    "# e) Training and validation NRMSE obtained using pseudo inverse with lambda2 [2]\n",
    "NRMSE=np.zeros([1,2])\n",
    "    \n",
    "    \n",
    "weight1,w0_bias,v_noise,data=gen_input(sample=1500,dimension=5)\n",
    "\n",
    "# adding a column of all ones to data for calculating the w0:\n",
    "data1=np.concatenate((np.ones([data.shape[0],1]),data),axis=1)\n",
    "\n",
    "#Calculating the target values\n",
    "target=gen_dep(data,weight1,w0_bias,v_noise)\n",
    "\n",
    "#splitting the data into training and validation\n",
    "train_tar_set=target[0:np.int32(np.round(0.75*target.shape[0])),:]\n",
    "val_tar_set=target[np.int32(np.round(0.75*target.shape[0]))+1:np.int32(np.round(0.85*target.shape[0])),:]\n",
    "train_set=data1[0:np.int32(np.round(0.75*data1.shape[0])),:]\n",
    "val_set=data1[np.int32(np.round(0.75*data1.shape[0]))+1:np.int32(np.round(0.85*data1.shape[0])),:]\n",
    "    \n",
    "# estimation of the weight and prediction for training data\n",
    "weight,predicted_y,mean_sq_error=Pinv_w_est (train_set,train_tar_set,lamb=10)\n",
    "NRMSE[0,0]=np.sqrt(mean_sq_error)/(np.max(train_tar_set)-np.min(train_tar_set))\n",
    "    \n",
    "# estimation of the weight and prediction for validation data\n",
    "weight2,predicted_y2,mean_sq_error2=Pinv_w_est (val_set,val_tar_set,lamb=10)\n",
    "NRMSE[0,1]=np.sqrt(mean_sq_error2)/(np.max(val_tar_set)-np.min(val_tar_set))\n",
    "print (NRMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3179ad",
   "metadata": {},
   "source": [
    "### Comments [e]:\n",
    "Including Lambda always not cause the increase of accuracy but sometimes can cause an increase to MSE value as in the case shown above. The NRMSE value has increased than the case without Lambda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "403737bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10, 10, 0.0009980201721191406), (10, 100, 0.05186104774475098), (10, 1000, 0.2184162139892578), (10, 10000, 371.92160415649414), (100, 10, 0.0009975433349609375), (100, 100, 0.05147266387939453), (100, 1000, 0.21442222595214844), (100, 10000, 367.3194601535797), (1000, 10, 0.000997304916381836), (1000, 100, 0.001997232437133789), (1000, 1000, 0.24335241317749023), (1000, 10000, 374.438360452652), (10000, 10, 0.0), (10000, 100, 0.05395793914794922), (10000, 1000, 0.2982017993927002), (10000, 10000, 385.43973994255066)]\n"
     ]
    }
   ],
   "source": [
    "# f) Time taken to solve pseudo inverse with number of samples and number of variables and its breaking points [2]\n",
    "\n",
    "# We have to import the time library to calcukate the time taken by the funtion\n",
    "import time\n",
    "\n",
    "\n",
    "# Generate a range of data sets with varying numbers of samples and variables\n",
    "samples = [10, 100, 1000, 10000]\n",
    "dimension = [10, 100, 1000, 10000]\n",
    "\n",
    "# Initialize an empty list to store the time taken for each calculation\n",
    "time_taken = []\n",
    "\n",
    "# Loop through the data sets and calculate the pseudo inverse\n",
    "for s in samples:\n",
    "    for d in dimension:\n",
    "        weight1,w0_bias,v_noise,data=gen_input(s,d)\n",
    "        target=gen_dep(data,weight1,w0_bias,v_noise)\n",
    "        start_time = time.time()\n",
    "        weight,predicted_y,mean_sq_error=Pinv_w_est (data,target,lamb=0.1)\n",
    "        end_time = time.time()\n",
    "        time_taken.append((s, d, end_time - start_time))\n",
    "\n",
    "# Print the time taken for each calculation\n",
    "print(time_taken)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff8018a",
   "metadata": {},
   "source": [
    "### Comments [f]:\n",
    "Time taken to solve a problem using pseudo inverse is affected highly by the number of features and slightly affected by the number of samples. The breaking point for the case above is 10X10000 which takes a computing time of 371 sec. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "abd9298b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x15f3c4e8460>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c4e8730>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c4f3850>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c4f3b20>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c503c40>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c503f10>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c51b070>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c51b340>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c529460>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c529730>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c533850>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c533b20>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c542c40>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c542f10>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c55a070>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c55a340>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c567460>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c567730>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c575850>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c575b20>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c580c40>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c580f10>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c59b070>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c59b340>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5a6460>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5a6730>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5b3850>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5b3b20>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5c1a30>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5c1d00>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5cee20>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5dc130>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x15f3c4e8a00>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c4e8cd0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c4f3df0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c503100>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c50d220>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c50d4f0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c51b610>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c51b8e0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c529a00>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c529cd0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c533df0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c542100>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c54e220>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c54e4f0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c55a610>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c55a8e0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c567a00>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c567cd0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c575df0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c580100>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c58b220>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c58b4f0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c59b610>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c59b8e0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5a6a00>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5a6cd0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5b3df0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5c10a0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5c1fd0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5ce2e0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5dc400>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5dc6d0>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x15f3c4e8190>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c4f3580>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c503970>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c50dd60>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c529190>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c533580>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c542970>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c54ed60>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c567190>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c575580>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c580970>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c58bd60>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5a6190>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5b3580>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5c1730>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5ceb50>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x15f3c4e8fa0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5033d0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c50d7c0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c51bbb0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c529fa0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5423d0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c54e7c0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c55abb0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c567fa0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5803d0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c58b7c0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c59bbb0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5a6fa0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5c1370>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5ce5b0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5dc9a0>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x15f3c4f32b0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5036a0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c50da90>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c51be80>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5332b0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5426a0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c54ea90>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c55ae80>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5752b0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5806a0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c58ba90>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c59be80>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5b32b0>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5c1640>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5ce880>,\n",
       "  <matplotlib.lines.Line2D at 0x15f3c5dcc70>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8beb40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16592564 0.25429432]\n",
      " [0.17164651 0.20610108]\n",
      " [0.14715312 0.1532433 ]\n",
      " [0.11920168 0.19905647]\n",
      " [0.15285858 0.18009218]\n",
      " [0.11878771 0.15449159]\n",
      " [0.10256524 0.13363059]]\n"
     ]
    }
   ],
   "source": [
    "# g) Training and validation NRMSE obtained using gradient descent with max_iter [2]\n",
    "max_iter=np.array([5,10,100,500,1000,1500,2000])\n",
    "NRMSE=np.zeros([max_iter.shape[0],2])\n",
    "for i in range(max_iter.shape[0]):\n",
    "    \n",
    "    # because of avoiding \"SyntaxError: positional argument follows keyword argument\" we have to predefine the value of \n",
    "    # max_iteration and store it in another value. While calling the function we can simply put max_iter=maxit\n",
    "    maxit=max_iter[i]\n",
    "    \n",
    "    # Generating the weight, bias, noise and data\n",
    "    weight1,w0_bias,v_noise,data=gen_input(sample=1000,dimension=20)\n",
    "    target=gen_dep(data,weight1,w0_bias,v_noise)\n",
    "    \n",
    "    #splitting the data into training and validation\n",
    "    train_tar_set=target[0:np.int32(np.round(0.75*target.shape[0])),:]\n",
    "    val_tar_set=target[np.int32(np.round(0.75*target.shape[0]))+1:np.int32(np.round(0.85*target.shape[0])),:]\n",
    "    train_set=data[0:np.int32(np.round(0.75*data.shape[0])),:]\n",
    "    val_set=data[np.int32(np.round(0.75*data.shape[0]))+1:np.int32(np.round(0.85*data.shape[0])),:]\n",
    "    \n",
    "    # estimation of the weight and prediction for training data\n",
    "    weight,predicted_y,NRMSE1=linear_reg_w(train_set,train_tar_set,lam2=0, lam1=0,eta=0.0001, max_iter=maxit, min_change_NRMSE=1e-6)\n",
    "    NRMSE[i,0]=NRMSE1\n",
    "    \n",
    "    # estimation of the weight and prediction for validation data\n",
    "    weight2,predicted_y2,NRMSE2=linear_reg_w(val_set,val_tar_set,lam2=0, lam1=0,eta=0.0001, max_iter=maxit, min_change_NRMSE=1e-6)\n",
    "    NRMSE[i,1]=NRMSE2\n",
    "    \n",
    "    \n",
    "    del weight1,w0_bias,v_noise,data,weight,predicted_y,NRMSE1,NRMSE2,maxit\n",
    "print(NRMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2f061",
   "metadata": {},
   "source": [
    "### Comments [g]:\n",
    "Number of iterations has in-direct impact on NRMSE value of the model. As the iteration values increases the NRMSE values decreases and the accuracy increases. Also for higher number of iterations the fitting of the model gets better and accuracy of training and validation test get closer to each otehr. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b37a043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05291556 0.06469915]\n",
      " [0.06916322 0.07339245]\n",
      " [0.17087182 0.18567415]\n",
      " [0.24239175 0.25040225]\n",
      " [0.06709947 0.08223507]\n",
      " [0.24387213 0.33707184]]\n"
     ]
    }
   ],
   "source": [
    "# h) Training and validation NRMSE obtained using gradient descent with eta [2]\n",
    "eta=np.array([0.1,0.001,0.0001,0.00001,0.000001,0.0000001])\n",
    "NRMSE=np.zeros([eta.shape[0],2])\n",
    "for i in range(eta.shape[0]):\n",
    "    \n",
    "    # because of avoiding \"SyntaxError: positional argument follows keyword argument\" we have to predefine the value of \n",
    "    # max_iteration and store it in another value. While calling the function we can simply put et=eta[i]\n",
    "    et=eta[i]\n",
    "    \n",
    "    # Generating the weight, bias, noise and data\n",
    "    weight1,w0_bias,v_noise,data=gen_input(sample=1000,dimension=4)\n",
    "    target=gen_dep(data,weight1,w0_bias,v_noise)\n",
    "    \n",
    "    #splitting the data into training and validation\n",
    "    train_tar_set=target[0:np.int32(np.round(0.75*target.shape[0])),:]\n",
    "    val_tar_set=target[np.int32(np.round(0.75*target.shape[0]))+1:np.int32(np.round(0.85*target.shape[0])),:]\n",
    "    train_set=data[0:np.int32(np.round(0.75*data.shape[0])),:]\n",
    "    val_set=data[np.int32(np.round(0.75*data.shape[0]))+1:np.int32(np.round(0.85*data.shape[0])),:]\n",
    "    \n",
    "    # estimation of the weight and prediction for training data\n",
    "    weight,predicted_y,NRMSE1=linear_reg_w(train_set,train_tar_set,lam2=0, lam1=0,eta=et, max_iter=1500, min_change_NRMSE=1e-6)\n",
    "    NRMSE[i,0]=NRMSE1\n",
    "    \n",
    "    # estimation of the weight and prediction for validation data\n",
    "    weight2,predicted_y2,NRMSE2=linear_reg_w(val_set,val_tar_set,lam2=0, lam1=0,eta=et, max_iter=1500, min_change_NRMSE=1e-6)\n",
    "    NRMSE[i,1]=NRMSE2\n",
    "    \n",
    "    \n",
    "    del weight1,w0_bias,v_noise,data,weight,predicted_y,NRMSE1,NRMSE2,et\n",
    "print(NRMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fda44c",
   "metadata": {},
   "source": [
    "### Comments [h]:\n",
    "The eta value or learning rate can impact the NRMSE value but is also dependent on number of max iterations. We cannot find the optimal number of eta or max iteration independent of each other. As, I run the code in \"h\" section for different number of max iterations I got different eta values for optimal NRMSE. So, I used the optimal number of max iterations we found in section \"g\" and in that case the optimal eta=0.1. As the eta values decrease the NRMSE values increase and then decrease. But the different with optimal eta is high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a20ddddf",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 745. GiB for an array with shape (1000000, 100000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m samples:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dimension:\n\u001b[1;32m---> 13\u001b[0m         weight1,w0_bias,v_noise,data\u001b[38;5;241m=\u001b[39m\u001b[43mgen_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m         target\u001b[38;5;241m=\u001b[39mgen_dep(data,weight1,w0_bias,v_noise)\n\u001b[0;32m     15\u001b[0m         start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mgen_input\u001b[1;34m(sample, dimension)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#for normalizing the noise value among different methods I have chosen to divide all the values by\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#the maximum value so the new values of v will me v/max(v)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m v_noise\u001b[38;5;241m=\u001b[39mv_noise\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mmax\u001b[39m(v_noise)\n\u001b[1;32m----> 8\u001b[0m data\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimension\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (weight1,w0_bias,v_noise,data)\n",
      "File \u001b[1;32mmtrand.pyx:1243\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randn\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mmtrand.pyx:1400\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.standard_normal\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_common.pyx:598\u001b[0m, in \u001b[0;36mnumpy.random._common.cont\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 745. GiB for an array with shape (1000000, 100000) and data type float64"
     ]
    }
   ],
   "source": [
    "# i) Time taken to solve gradient descent with number of samples and number of variables and its breaking points [2]\n",
    "\n",
    "# Generate a range of data sets with varying numbers of samples and variables\n",
    "samples = [10, 100, 1000, 10000,100000,1000000]\n",
    "dimension = [10, 100, 1000, 10000,100000]\n",
    "\n",
    "# Initialize an empty list to store the time taken for each calculation\n",
    "time_taken = []\n",
    "\n",
    "# Loop through the data sets and calculate the gradient descent\n",
    "for s in samples:\n",
    "    for d in dimension:\n",
    "        weight1,w0_bias,v_noise,data=gen_input(s,d)\n",
    "        target=gen_dep(data,weight1,w0_bias,v_noise)\n",
    "        start_time = time.time()\n",
    "        weight,predicted_y,NRMSE1=linear_reg_w(train_set,train_tar_set,lam2=0, lam1=0,eta=0.0001, max_iter=1000,\n",
    "                                               min_change_NRMSE=1e-6)\n",
    "        end_time = time.time()\n",
    "        time_taken.append((s, d, end_time - start_time))\n",
    "\n",
    "# Print the time taken for each calculation\n",
    "print(time_taken)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "97ba3429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1000000, 10, 0.04288601875305176), (1000000, 100, 0.05186104774475098), (1000000, 1000, 0.054852962493896484), (1000000, 10000, 0.041886329650878906), (1000000, 100000, 0.04288673400878906), (1000000, 1000000, 0.10372328758239746)]\n"
     ]
    }
   ],
   "source": [
    "# j) Time taken to solve gradient descent with number of variables and its breaking point [2]\n",
    "\n",
    "# specify the number of different variables\n",
    "dimension = [10, 100, 1000, 10000,100000,1000000]\n",
    "\n",
    "# Initialize an empty list to store the time taken for each calculation\n",
    "time_taken = []\n",
    "\n",
    "# Loop through the data sets and calculate the gradient descent\n",
    "for d in dimension:\n",
    "    weight1,w0_bias,v_noise,data=gen_input(10000,d)\n",
    "    target=gen_dep(data,weight1,w0_bias,v_noise)\n",
    "    start_time = time.time()\n",
    "    weight,predicted_y,NRMSE1=linear_reg_w(train_set,train_tar_set,lam2=0, lam1=0,eta=0.0001, max_iter=1000,\n",
    "                                            min_change_NRMSE=1e-6)\n",
    "    end_time = time.time()\n",
    "    time_taken.append((s, d, end_time - start_time))\n",
    "\n",
    "# Print the time taken for each calculation\n",
    "print(time_taken)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6dc401a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# k) Training and validation NRMSE and number of nearly zero weights obtained using gradient descent with lambda2 [2]\n",
    "\n",
    "lamb_two=np.array([0.01,0.1,1,10,100])\n",
    "NRMSE=np.zeros([lamb_two.shape[0],2])\n",
    "for i in range(lamb_two.shape[0]):\n",
    "    \n",
    "    # because of avoiding \"SyntaxError: positional argument follows keyword argument\" we have to predefine the value of \n",
    "    # max_iteration and store it in another value. While calling the function we can simply put et=eta[i]\n",
    "    lam=lamb_two[i]\n",
    "    \n",
    "    # Generating the weight, bias, noise and data\n",
    "    weight1,w0_bias,v_noise,data=gen_input(sample=1000,dimension=4)\n",
    "    target=gen_dep(data,weight1,w0_bias,v_noise)\n",
    "    \n",
    "    #splitting the data into training and validation\n",
    "    train_tar_set=target[0:np.int32(np.round(0.75*target.shape[0])),:]\n",
    "    val_tar_set=target[np.int32(np.round(0.75*target.shape[0]))+1:np.int32(np.round(0.85*target.shape[0])),:]\n",
    "    train_set=data[0:np.int32(np.round(0.75*data.shape[0])),:]\n",
    "    val_set=data[np.int32(np.round(0.75*data.shape[0]))+1:np.int32(np.round(0.85*data.shape[0])),:]\n",
    "    \n",
    "    # estimation of the weight and prediction for training data\n",
    "    weight,predicted_y,NRMSE1=linear_reg_w(train_set,train_tar_set,lam2=lam, lam1=0,eta=0.001, max_iter=1000, min_change_NRMSE=1e-6)\n",
    "    NRMSE[i,0]=NRMSE1\n",
    "    \n",
    "    # estimation of the weight and prediction for validation data\n",
    "    weight2,predicted_y2,NRMSE2=linear_reg_w(val_set,val_tar_set,lam2=lam, lam1=0,eta=0.001, max_iter=1000, min_change_NRMSE=1e-6)\n",
    "    NRMSE[i,1]=NRMSE2\n",
    "    \n",
    "    \n",
    "    del weight1,w0_bias,v_noise,data,weight,predicted_y,NRMSE1,NRMSE2,lam\n",
    "\n",
    "\n",
    "# Counting number of NRMSE values which is closer to zero. In this case any number which is smaller than 0.1, we will\n",
    "# cosider it as closer to zero\n",
    "countt=np.zeros([1,2])\n",
    "countt[0,0]=np.count_nonzero(NRMSE[:,0]<0.1)\n",
    "countt[0,1]=np.count_nonzero(NRMSE[:,1]<0.1)\n",
    "print (countt)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6d006864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "#l) Training and validation NRMSE and number of nearly zero weights obtained using gradient descent with lambda1 [2]\n",
    "\n",
    "lamb_one=np.array([0.01,0.1,1,10,100])\n",
    "NRMSE=np.zeros([lamb_one.shape[0],2])\n",
    "for i in range(lamb_one.shape[0]):\n",
    "    \n",
    "    # because of avoiding \"SyntaxError: positional argument follows keyword argument\" we have to predefine the value of \n",
    "    # max_iteration and store it in another value. While calling the function we can simply put et=eta[i]\n",
    "    lam=lamb_one[i]\n",
    "    \n",
    "    # Generating the weight, bias, noise and data\n",
    "    weight1,w0_bias,v_noise,data=gen_input(sample=1000,dimension=4)\n",
    "    target=gen_dep(data,weight1,w0_bias,v_noise)\n",
    "    \n",
    "    #splitting the data into training and validation\n",
    "    train_tar_set=target[0:np.int32(np.round(0.75*target.shape[0])),:]\n",
    "    val_tar_set=target[np.int32(np.round(0.75*target.shape[0]))+1:np.int32(np.round(0.85*target.shape[0])),:]\n",
    "    train_set=data[0:np.int32(np.round(0.75*data.shape[0])),:]\n",
    "    val_set=data[np.int32(np.round(0.75*data.shape[0]))+1:np.int32(np.round(0.85*data.shape[0])),:]\n",
    "    \n",
    "    # estimation of the weight and prediction for training data\n",
    "    weight,predicted_y,NRMSE1=linear_reg_w(train_set,train_tar_set,lam2=0, lam1=lam,eta=0.001, max_iter=1000, min_change_NRMSE=1e-6)\n",
    "    NRMSE[i,0]=NRMSE1\n",
    "    \n",
    "    # estimation of the weight and prediction for validation data\n",
    "    weight2,predicted_y2,NRMSE2=linear_reg_w(val_set,val_tar_set,lam2=0, lam1=lam,eta=0.001, max_iter=1000, min_change_NRMSE=1e-6)\n",
    "    NRMSE[i,1]=NRMSE2\n",
    "    \n",
    "    \n",
    "    del weight1,w0_bias,v_noise,data,weight,predicted_y,NRMSE1,NRMSE2,lam\n",
    "\n",
    "\n",
    "# Counting number of NRMSE values which is closer to zero. In this case any number which is smaller than 0.001, we will\n",
    "# cosider it as closer to zero\n",
    "countt=np.zeros([1,2])\n",
    "countt[0,0]=np.count_nonzero(NRMSE[:,0]<0.1)\n",
    "countt[0,1]=np.count_nonzero(NRMSE[:,1]<0.1)\n",
    "print (countt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d79f9334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m) Training and validation NRMSE for optimal lambda2 with noise variance [2]\n",
    "\n",
    "# Re-writing the function of gradient descent with variance\n",
    "\n",
    "def linear_reg_noise(dt,t,noise, eta, max_iter, min_change_NRMSE,lam2, lam1):\n",
    "    n, d = dt.shape\n",
    "    weight = np.ones([d,1])\n",
    "    for i in range(max_iter):\n",
    "        y = dt.dot(weight)+noise\n",
    "        grad = (dt.T @ (y - t))/dt.shape[0] + lam2 * np.square(weight)  + lam1 * np.abs(weight)\n",
    "        weight -= eta * grad\n",
    "        NRMSE = np.sqrt(np.mean(np.square(y - t))) / (np.max(t) - np.min(t))\n",
    "        if NRMSE < min_change_NRMSE:\n",
    "            break\n",
    "    return weight,y, NRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85951566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IIT BOMBAY\\AppData\\Local\\Temp\\ipykernel_2752\\2083073867.py:10: RuntimeWarning: overflow encountered in square\n",
      "  grad = (dt.T @ (y - t))/dt.shape[0] + lam2 * np.square(weight)  + lam1 * np.abs(weight)\n",
      "C:\\Users\\IIT BOMBAY\\AppData\\Local\\Temp\\ipykernel_2752\\2083073867.py:12: RuntimeWarning: overflow encountered in square\n",
      "  NRMSE = np.sqrt(np.mean(np.square(y - t))) / (np.max(t) - np.min(t))\n",
      "C:\\Users\\IIT BOMBAY\\AppData\\Local\\Temp\\ipykernel_2752\\2083073867.py:10: RuntimeWarning: invalid value encountered in multiply\n",
      "  grad = (dt.T @ (y - t))/dt.shape[0] + lam2 * np.square(weight)  + lam1 * np.abs(weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12334023]\n",
      " [0.12333478]\n",
      " [0.12328527]\n",
      " [0.12279848]\n",
      " [0.11868472]\n",
      " [0.1118667 ]\n",
      " [0.1461382 ]\n",
      " [0.16954201]\n",
      " [0.17870773]\n",
      " [       nan]\n",
      " [       nan]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15f3c7ba340>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#finding the optimal lambda 2 value\n",
    "lambda2=np.array([0.000001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000,100000])\n",
    "NRMSE=np.zeros([lambda2.shape[0],1])\n",
    "\n",
    "# Generating the weight, bias, noise and data\n",
    "weight1,w0_bias,v_noise,data=gen_input(sample=1000,dimension=4)\n",
    "target=gen_dep(data,weight1,w0_bias,v_noise)\n",
    "\n",
    "for i in range(lambda2.shape[0]):\n",
    "    lamm=lambda2[i]\n",
    "    w,y,NRMSE[i,0]=linear_reg_noise(data,target,v_noise, eta=0.001, max_iter=1000, min_change_NRMSE=0.005,lam2=lamm, lam1=0)\n",
    "    del w,y\n",
    "print(NRMSE)\n",
    "np.where(NRMSE==min(NRMSE))\n",
    "plt.plot(lambda2,NRMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef03a3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13318921 0.13695542]]\n"
     ]
    }
   ],
   "source": [
    "# the optimal lambda2 for the case above is 1000. so, we take lambda 2 as 1000. \n",
    "\n",
    "NRMSE=np.zeros([1,2])    \n",
    "    \n",
    "#splitting the data into training and validation\n",
    "train_tar_set=target[0:np.int32(np.round(0.75*target.shape[0])),:]\n",
    "val_tar_set=target[np.int32(np.round(0.75*target.shape[0]))+1:np.int32(np.round(0.85*target.shape[0])),:]\n",
    "train_set=data[0:np.int32(np.round(0.75*data.shape[0])),:]\n",
    "val_set=data[np.int32(np.round(0.75*data.shape[0]))+1:np.int32(np.round(0.85*data.shape[0])),:]\n",
    "    \n",
    "# estimation of the weight and prediction for training data\n",
    "weight,predicted_y,NRMSE1=linear_reg_noise(train_set,train_tar_set,noise[:750,:], eta=0.001, max_iter=1000, min_change_NRMSE=0.005,lam2=1, lam1=0)\n",
    "NRMSE[0,0]=NRMSE1\n",
    "    \n",
    "# estimation of the weight and prediction for validation data\n",
    "weight2,predicted_y2,NRMSE2=linear_reg_noise(val_set,val_tar_set,noise[750:849,:], eta=0.001, max_iter=1000, min_change_NRMSE=0.005,lam2=1, lam1=0)\n",
    "NRMSE[0,1]=NRMSE2\n",
    "    \n",
    "    \n",
    "del weight1,w0_bias,v_noise,data,weight,predicted_y,NRMSE1,NRMSE2\n",
    "print(NRMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "94f24def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12628834]\n",
      " [0.12628206]\n",
      " [0.12622503]\n",
      " [0.12566399]\n",
      " [0.12094905]\n",
      " [0.12520782]\n",
      " [0.1869219 ]\n",
      " [0.19800059]\n",
      " [       nan]\n",
      " [       nan]\n",
      " [       nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IIT BOMBAY\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "C:\\Users\\IIT BOMBAY\\AppData\\Local\\Temp\\ipykernel_2752\\2083073867.py:12: RuntimeWarning: overflow encountered in square\n",
      "  NRMSE = np.sqrt(np.mean(np.square(y - t))) / (np.max(t) - np.min(t))\n",
      "C:\\Users\\IIT BOMBAY\\AppData\\Local\\Temp\\ipykernel_2752\\2083073867.py:10: RuntimeWarning: overflow encountered in square\n",
      "  grad = (dt.T @ (y - t))/dt.shape[0] + lam2 * np.square(weight)  + lam1 * np.abs(weight)\n",
      "C:\\Users\\IIT BOMBAY\\AppData\\Local\\Temp\\ipykernel_2752\\2083073867.py:10: RuntimeWarning: invalid value encountered in multiply\n",
      "  grad = (dt.T @ (y - t))/dt.shape[0] + lam2 * np.square(weight)  + lam1 * np.abs(weight)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15f407589d0>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# n) Training and validation NRMSE for optimal lambda1 with noise variance [2]\n",
    "\n",
    "#finding the optimal lambda 2 value\n",
    "lambda1=np.array([0.000001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000,100000])\n",
    "NRMSE=np.zeros([lambda1.shape[0],1])\n",
    "\n",
    "# Generating the weight, bias, noise and data\n",
    "weight1,w0_bias,v_noise,data=gen_input(sample=1000,dimension=4)\n",
    "target=gen_dep(data,weight1,w0_bias,v_noise)\n",
    "\n",
    "for i in range(lambda2.shape[0]):\n",
    "    lamm=lambda1[i]\n",
    "    w,y,NRMSE[i,0]=linear_reg_noise(data,target,v_noise, eta=0.001, max_iter=1000, min_change_NRMSE=0.005,lam2=0, lam1=lamm)\n",
    "    del w,y\n",
    "print(NRMSE)\n",
    "plt.plot(lambda1,NRMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3382a44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14084925 0.17153129]]\n"
     ]
    }
   ],
   "source": [
    "# the optimal lambda1 for the case above is 0.1. so, we take lambda 2 as 1000. \n",
    "\n",
    "NRMSE=np.zeros([1,2])    \n",
    "    \n",
    "#splitting the data into training and validation\n",
    "train_tar_set=target[0:np.int32(np.round(0.75*target.shape[0])),:]\n",
    "val_tar_set=target[np.int32(np.round(0.75*target.shape[0]))+1:np.int32(np.round(0.85*target.shape[0])),:]\n",
    "train_set=data[0:np.int32(np.round(0.75*data.shape[0])),:]\n",
    "val_set=data[np.int32(np.round(0.75*data.shape[0]))+1:np.int32(np.round(0.85*data.shape[0])),:]\n",
    "    \n",
    "# estimation of the weight and prediction for training data\n",
    "weight,predicted_y,NRMSE1=linear_reg_noise(train_set,train_tar_set,noise[:750,:], eta=0.001, max_iter=1000, min_change_NRMSE=0.005,lam2=0, lam1=0.1)\n",
    "NRMSE[0,0]=NRMSE1\n",
    "    \n",
    "# estimation of the weight and prediction for validation data\n",
    "weight2,predicted_y2,NRMSE2=linear_reg_noise(val_set,val_tar_set,noise[750:849,:], eta=0.001, max_iter=1000, min_change_NRMSE=0.005,lam2=0, lam1=0.1)\n",
    "NRMSE[0,1]=NRMSE2\n",
    "    \n",
    "    \n",
    "del weight1,w0_bias,v_noise,data,weight,predicted_y,NRMSE1,NRMSE2\n",
    "print(NRMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "551601cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 745. GiB for an array with shape (100000, 1000000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m samples:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dimension:\n\u001b[1;32m---> 14\u001b[0m         weight1,w0_bias,v_noise,data\u001b[38;5;241m=\u001b[39m\u001b[43mgen_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m         target\u001b[38;5;241m=\u001b[39mgen_dep(data,weight1,w0_bias,v_noise)\n\u001b[0;32m     16\u001b[0m         start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mgen_input\u001b[1;34m(sample, dimension)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#for normalizing the noise value among different methods I have chosen to divide all the values by\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#the maximum value so the new values of v will me v/max(v)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m v_noise\u001b[38;5;241m=\u001b[39mv_noise\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mmax\u001b[39m(v_noise)\n\u001b[1;32m----> 8\u001b[0m data\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimension\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (weight1,w0_bias,v_noise,data)\n",
      "File \u001b[1;32mmtrand.pyx:1243\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randn\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mmtrand.pyx:1400\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.standard_normal\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_common.pyx:598\u001b[0m, in \u001b[0;36mnumpy.random._common.cont\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 745. GiB for an array with shape (100000, 1000000) and data type float64"
     ]
    }
   ],
   "source": [
    "# o) Experiment (f) but, this time with number of training samples and number of variables [2]\n",
    "\n",
    "# Generate a range of data sets with varying numbers of samples and variables\n",
    "\n",
    "samples = [10, 100, 1000, 10000,100000,1000000]\n",
    "dimension = [10, 100, 1000, 10000,100000,1000000]\n",
    "\n",
    "# Initialize an empty list to store the time taken for each calculation\n",
    "time_taken = []\n",
    "\n",
    "# Loop through the data sets and calculate the gradient descent\n",
    "for s in samples:\n",
    "    for d in dimension:\n",
    "        weight1,w0_bias,v_noise,data=gen_input(s,d)\n",
    "        target=gen_dep(data,weight1,w0_bias,v_noise)\n",
    "        start_time = time.time()\n",
    "        weight,predicted_y,NRMSE1=linear_reg_w(train_set,train_tar_set,lam2=0, lam1=0,eta=0.0001, max_iter=1000,\n",
    "                                               min_change_NRMSE=1e-6)\n",
    "        end_time = time.time()\n",
    "        time_taken.append((s, d, end_time - start_time))\n",
    "\n",
    "# Print the time taken for each calculation\n",
    "print(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8885e94f",
   "metadata": {},
   "source": [
    "# Overall understanding:\n",
    "using a function and understanding a function specially writing a function is totally different. I have used linear regression functions earlier in other problems which was available in libraries but this time my understanding from linear regression was much more deeper and it was a very good experience for me to first understand the mathematics behind all these models and then code them in Python. Now I am very confident about linear regression, gradient descent, pseudo inverse and regularization concepts. \n",
    "\n",
    "According to the analysis done in the above assignment Pseudo-inverse method was working better than Gradient-descent method in my case, while in can be not correct in general. Including W0 in the weight calculation of Pseudo-inverse method has a huge impact on accuracy of the method and it increases significantly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77548457",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
